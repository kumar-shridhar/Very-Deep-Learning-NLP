{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 (NLP): Very Deep Learning\n",
    "\n",
    "**Natural language processing (NLP)** is the ability of a computer program to understand human language as it is spoken. It involves a pipeline of steps and by the end of the exercise, we would be able to classify the sentiment of a given review as POSITIVE or NEGATIVE.\n",
    "\n",
    "\n",
    "Before starting, it is important to understand the need for RNNs and the lecture from Stanford is a must to see before starting the exercise:\n",
    "\n",
    "https://www.youtube.com/watch?v=iX5V1WpxxkY\n",
    "\n",
    "When done, let's begin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this exercise, we will import libraries when needed so that we understand the need for it. \n",
    "# However, this is a bad practice and don't get used to it.\n",
    "import numpy as np\n",
    "\n",
    "# read data from reviews and labels file.\n",
    "with open('data/reviews.txt', 'r') as f:\n",
    "    reviews_ = f.readlines()\n",
    "with open('data/labels.txt', 'r') as f:\n",
    "    \n",
    "    labels = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n",
      "\t: bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life...\n",
      "negative\n",
      "\t: story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terr...\n",
      "positive\n",
      "\t: homelessness  or houselessness as george carlin stated  has been an issue for years but never a plan...\n",
      "negative\n",
      "\t: airport    starts as a brand new luxury    plane is loaded up with valuable paintings  such belongin...\n",
      "positive\n",
      "\t: brilliant over  acting by lesley ann warren . best dramatic hobo lady i have ever seen  and love sce...\n"
     ]
    }
   ],
   "source": [
    "# One of the most important task is to visualize data before starting with any ML task. \n",
    "for i in range(5):\n",
    "    print(labels[i] + \"\\t: \" + reviews_[i][:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We can see there are a lot of punctuation marks like fullstop(.), comma(,), new line (\\n) and so on and we need to remove it. \n",
    "\n",
    "Here is a list of all the punctuation marks that needs to be removed \n",
    "```\n",
    "(!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Remove all the punctuation marks from the reviews.\n",
    "Many ways of doing it: Regex, Spacy, import punctuation from string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make everything lower case to make the whole dataset even. \n",
    "reviews = ''.join(reviews_).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the function below to remove punctuations and save it in no_punct_text\n",
    "\n",
    "def text_without_punct(reviews):\n",
    "    return ''.join([i for i in reviews if i not in punctuation])\n",
    "\n",
    "no_punct_text = text_without_punct(reviews)\n",
    "reviews_split = no_punct_text.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_punct_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the formatted no_punct_text into words\n",
    "def split_in_words(no_punct_text):\n",
    "    return no_punct_text.split()\n",
    "\n",
    "words = split_in_words(no_punct_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# once you are done print the ten words that should yield the following output\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6020196"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the total length of the words\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74072"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of unique words\n",
    "len(set(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next step is to create a vocabulary. This way every word is mapped to an integer number.\n",
    "```\n",
    "Example: 1: hello, 2: I, 3: am, 4: Robo and so on...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a vocab out of it\n",
    "\n",
    "# feel free to use this import \n",
    "from collections import Counter\n",
    "\n",
    "## Let's keep a count of all the words and let's see how many words are there. \n",
    "def word_count(words):\n",
    "    return Counter(words)\n",
    "\n",
    "counts=word_count(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1658\n",
      "9308\n"
     ]
    }
   ],
   "source": [
    "# If you did everything correct, this is what you should get as output. \n",
    "print (counts['wonderful'])\n",
    "\n",
    "print (counts['bad'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Word to Integer and Integer to word\n",
    "The task is to map every word to an integer value and then vice-versa. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'and'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a vocabulary for the words\n",
    "def vocabulary(counts):\n",
    "    return sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "vocab = vocabulary(counts)\n",
    "print(len(vocab))\n",
    "vocab[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map each vocab word to an integer. Also, start the indexing with 1 as we will use \n",
    "# '0' for padding and we dont want to mix the two.\n",
    "def vocabulary_to_integer(vocab):\n",
    "    return {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "vocab_to_int = vocabulary_to_integer(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify if the length is same and if 'and' is mapped to the correct integer value.\n",
    "print(len(vocab_to_int))\n",
    "vocab_to_int['and']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what positve words in positive reviews we have and what we have in negative reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_counts = Counter()\n",
    "negative_counts = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(reviews_)):\n",
    "    if(labels[i] == 'positive\\n'):\n",
    "        for word in reviews_[i].split(\" \"):\n",
    "            positive_counts[word] += 1\n",
    "    else:\n",
    "        for word in reviews_[i].split(\" \"):\n",
    "            negative_counts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " 'positive\\n',\n",
       " 'negative\\n',\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 537968),\n",
       " ('the', 173324),\n",
       " ('.', 159654),\n",
       " ('and', 89722),\n",
       " ('a', 83688),\n",
       " ('of', 76855),\n",
       " ('to', 66746),\n",
       " ('is', 57245),\n",
       " ('in', 50215),\n",
       " ('br', 49235),\n",
       " ('it', 48025),\n",
       " ('i', 40743),\n",
       " ('that', 35630),\n",
       " ('this', 35080),\n",
       " ('s', 33815),\n",
       " ('as', 26308),\n",
       " ('with', 23247),\n",
       " ('for', 22416),\n",
       " ('was', 21917),\n",
       " ('film', 20937),\n",
       " ('but', 20822),\n",
       " ('movie', 19074),\n",
       " ('his', 17227),\n",
       " ('on', 17008),\n",
       " ('you', 16681),\n",
       " ('he', 16282),\n",
       " ('are', 14807),\n",
       " ('not', 14272),\n",
       " ('t', 13720),\n",
       " ('one', 13655),\n",
       " ('have', 12587),\n",
       " ('\\n', 12500),\n",
       " ('be', 12416),\n",
       " ('by', 11997),\n",
       " ('all', 11942),\n",
       " ('who', 11464),\n",
       " ('an', 11294),\n",
       " ('at', 11234),\n",
       " ('from', 10767),\n",
       " ('her', 10474),\n",
       " ('they', 9895),\n",
       " ('has', 9186),\n",
       " ('so', 9154),\n",
       " ('like', 9038),\n",
       " ('about', 8313),\n",
       " ('very', 8305),\n",
       " ('out', 8134),\n",
       " ('there', 8057),\n",
       " ('she', 7779),\n",
       " ('what', 7737),\n",
       " ('or', 7732),\n",
       " ('good', 7720),\n",
       " ('more', 7521),\n",
       " ('when', 7456),\n",
       " ('some', 7441),\n",
       " ('if', 7285),\n",
       " ('just', 7152),\n",
       " ('can', 7001),\n",
       " ('story', 6780),\n",
       " ('time', 6515),\n",
       " ('my', 6488),\n",
       " ('great', 6419),\n",
       " ('well', 6405),\n",
       " ('up', 6321),\n",
       " ('which', 6267),\n",
       " ('their', 6107),\n",
       " ('see', 6026),\n",
       " ('also', 5550),\n",
       " ('we', 5531),\n",
       " ('really', 5476),\n",
       " ('would', 5400),\n",
       " ('will', 5218),\n",
       " ('me', 5167),\n",
       " ('had', 5148),\n",
       " ('only', 5137),\n",
       " ('him', 5018),\n",
       " ('even', 4964),\n",
       " ('most', 4864),\n",
       " ('other', 4858),\n",
       " ('were', 4782),\n",
       " ('first', 4755),\n",
       " ('than', 4736),\n",
       " ('much', 4685),\n",
       " ('its', 4622),\n",
       " ('no', 4574),\n",
       " ('into', 4544),\n",
       " ('people', 4479),\n",
       " ('best', 4319),\n",
       " ('love', 4301),\n",
       " ('get', 4272),\n",
       " ('how', 4213),\n",
       " ('life', 4199),\n",
       " ('been', 4189),\n",
       " ('because', 4079),\n",
       " ('way', 4036),\n",
       " ('do', 3941),\n",
       " ('made', 3823),\n",
       " ('films', 3813),\n",
       " ('them', 3805),\n",
       " ('after', 3800),\n",
       " ('many', 3766),\n",
       " ('two', 3733),\n",
       " ('too', 3659),\n",
       " ('think', 3655),\n",
       " ('movies', 3586),\n",
       " ('characters', 3560),\n",
       " ('character', 3514),\n",
       " ('don', 3468),\n",
       " ('man', 3460),\n",
       " ('show', 3432),\n",
       " ('watch', 3424),\n",
       " ('seen', 3414),\n",
       " ('then', 3358),\n",
       " ('little', 3341),\n",
       " ('still', 3340),\n",
       " ('make', 3303),\n",
       " ('could', 3237),\n",
       " ('never', 3226),\n",
       " ('being', 3217),\n",
       " ('where', 3173),\n",
       " ('does', 3069),\n",
       " ('over', 3017),\n",
       " ('any', 3002),\n",
       " ('while', 2899),\n",
       " ('know', 2833),\n",
       " ('did', 2790),\n",
       " ('years', 2758),\n",
       " ('here', 2740),\n",
       " ('ever', 2734),\n",
       " ('end', 2696),\n",
       " ('these', 2694),\n",
       " ('such', 2590),\n",
       " ('real', 2568),\n",
       " ('scene', 2567),\n",
       " ('back', 2547),\n",
       " ('those', 2485),\n",
       " ('though', 2475),\n",
       " ('off', 2463),\n",
       " ('new', 2458),\n",
       " ('your', 2453),\n",
       " ('go', 2440),\n",
       " ('acting', 2437),\n",
       " ('plot', 2432),\n",
       " ('world', 2429),\n",
       " ('scenes', 2427),\n",
       " ('say', 2414),\n",
       " ('through', 2409),\n",
       " ('makes', 2390),\n",
       " ('better', 2381),\n",
       " ('now', 2368),\n",
       " ('work', 2346),\n",
       " ('young', 2343),\n",
       " ('old', 2311),\n",
       " ('ve', 2307),\n",
       " ('find', 2272),\n",
       " ('both', 2248),\n",
       " ('before', 2177),\n",
       " ('us', 2162),\n",
       " ('again', 2158),\n",
       " ('series', 2153),\n",
       " ('quite', 2143),\n",
       " ('something', 2135),\n",
       " ('cast', 2133),\n",
       " ('should', 2121),\n",
       " ('part', 2098),\n",
       " ('always', 2088),\n",
       " ('lot', 2087),\n",
       " ('another', 2075),\n",
       " ('actors', 2047),\n",
       " ('director', 2040),\n",
       " ('family', 2032),\n",
       " ('between', 2016),\n",
       " ('own', 2016),\n",
       " ('m', 1998),\n",
       " ('may', 1997),\n",
       " ('same', 1972),\n",
       " ('role', 1967),\n",
       " ('watching', 1966),\n",
       " ('every', 1954),\n",
       " ('funny', 1953),\n",
       " ('doesn', 1935),\n",
       " ('performance', 1928),\n",
       " ('few', 1918),\n",
       " ('bad', 1907),\n",
       " ('look', 1900),\n",
       " ('re', 1884),\n",
       " ('why', 1855),\n",
       " ('things', 1849),\n",
       " ('times', 1832),\n",
       " ('big', 1815),\n",
       " ('however', 1795),\n",
       " ('actually', 1790),\n",
       " ('action', 1789),\n",
       " ('going', 1783),\n",
       " ('bit', 1757),\n",
       " ('comedy', 1742),\n",
       " ('down', 1740),\n",
       " ('music', 1738),\n",
       " ('must', 1728),\n",
       " ('take', 1709),\n",
       " ('saw', 1692),\n",
       " ('long', 1690),\n",
       " ('right', 1688),\n",
       " ('fun', 1686),\n",
       " ('fact', 1684),\n",
       " ('excellent', 1683),\n",
       " ('around', 1674),\n",
       " ('didn', 1672),\n",
       " ('without', 1671),\n",
       " ('thing', 1662),\n",
       " ('thought', 1639),\n",
       " ('got', 1635),\n",
       " ('each', 1630),\n",
       " ('day', 1614),\n",
       " ('feel', 1597),\n",
       " ('seems', 1596),\n",
       " ('come', 1594),\n",
       " ('done', 1586),\n",
       " ('beautiful', 1580),\n",
       " ('especially', 1572),\n",
       " ('played', 1571),\n",
       " ('almost', 1566),\n",
       " ('want', 1562),\n",
       " ('yet', 1556),\n",
       " ('give', 1553),\n",
       " ('pretty', 1549),\n",
       " ('last', 1543),\n",
       " ('since', 1519),\n",
       " ('different', 1504),\n",
       " ('although', 1501),\n",
       " ('gets', 1490),\n",
       " ('true', 1487),\n",
       " ('interesting', 1481),\n",
       " ('job', 1470),\n",
       " ('enough', 1455),\n",
       " ('our', 1454),\n",
       " ('shows', 1447),\n",
       " ('horror', 1441),\n",
       " ('woman', 1439),\n",
       " ('tv', 1400),\n",
       " ('probably', 1398),\n",
       " ('father', 1395),\n",
       " ('original', 1393),\n",
       " ('girl', 1390),\n",
       " ('point', 1379),\n",
       " ('plays', 1378),\n",
       " ('wonderful', 1372),\n",
       " ('far', 1358),\n",
       " ('course', 1358),\n",
       " ('john', 1350),\n",
       " ('rather', 1340),\n",
       " ('isn', 1328),\n",
       " ('ll', 1326),\n",
       " ('later', 1324),\n",
       " ('dvd', 1324),\n",
       " ('whole', 1310),\n",
       " ('war', 1310),\n",
       " ('d', 1307),\n",
       " ('found', 1306),\n",
       " ('away', 1306),\n",
       " ('screen', 1305),\n",
       " ('nothing', 1300),\n",
       " ('year', 1297),\n",
       " ('once', 1296),\n",
       " ('hard', 1294),\n",
       " ('together', 1280),\n",
       " ('set', 1277),\n",
       " ('am', 1277),\n",
       " ('having', 1266),\n",
       " ('making', 1265),\n",
       " ('place', 1263),\n",
       " ('might', 1260),\n",
       " ('comes', 1260),\n",
       " ('sure', 1253),\n",
       " ('american', 1248),\n",
       " ('play', 1245),\n",
       " ('kind', 1244),\n",
       " ('perfect', 1242),\n",
       " ('takes', 1242),\n",
       " ('performances', 1237),\n",
       " ('himself', 1230),\n",
       " ('worth', 1221),\n",
       " ('everyone', 1221),\n",
       " ('anyone', 1214),\n",
       " ('actor', 1203),\n",
       " ('three', 1201),\n",
       " ('wife', 1196),\n",
       " ('classic', 1192),\n",
       " ('goes', 1186),\n",
       " ('ending', 1178),\n",
       " ('version', 1168),\n",
       " ('star', 1149),\n",
       " ('enjoy', 1146),\n",
       " ('book', 1142),\n",
       " ('nice', 1132),\n",
       " ('everything', 1128),\n",
       " ('during', 1124),\n",
       " ('put', 1118),\n",
       " ('seeing', 1111),\n",
       " ('least', 1102),\n",
       " ('house', 1100),\n",
       " ('high', 1095),\n",
       " ('watched', 1094),\n",
       " ('loved', 1087),\n",
       " ('men', 1087),\n",
       " ('night', 1082),\n",
       " ('anything', 1075),\n",
       " ('believe', 1071),\n",
       " ('guy', 1071),\n",
       " ('top', 1063),\n",
       " ('amazing', 1058),\n",
       " ('hollywood', 1056),\n",
       " ('looking', 1053),\n",
       " ('main', 1044),\n",
       " ('definitely', 1043),\n",
       " ('gives', 1031),\n",
       " ('home', 1029),\n",
       " ('seem', 1028),\n",
       " ('episode', 1023),\n",
       " ('audience', 1020),\n",
       " ('sense', 1020),\n",
       " ('truly', 1017),\n",
       " ('special', 1011),\n",
       " ('second', 1009),\n",
       " ('short', 1009),\n",
       " ('fan', 1009),\n",
       " ('mind', 1005),\n",
       " ('human', 1001),\n",
       " ('recommend', 999),\n",
       " ('full', 996),\n",
       " ('black', 995),\n",
       " ('help', 991),\n",
       " ('along', 989),\n",
       " ('trying', 987),\n",
       " ('small', 986),\n",
       " ('death', 985),\n",
       " ('friends', 981),\n",
       " ('remember', 974),\n",
       " ('often', 970),\n",
       " ('said', 966),\n",
       " ('favorite', 962),\n",
       " ('heart', 959),\n",
       " ('early', 957),\n",
       " ('left', 956),\n",
       " ('until', 955),\n",
       " ('script', 954),\n",
       " ('let', 954),\n",
       " ('maybe', 937),\n",
       " ('today', 936),\n",
       " ('live', 934),\n",
       " ('less', 934),\n",
       " ('moments', 933),\n",
       " ('others', 929),\n",
       " ('brilliant', 926),\n",
       " ('shot', 925),\n",
       " ('liked', 923),\n",
       " ('become', 916),\n",
       " ('won', 915),\n",
       " ('used', 910),\n",
       " ('style', 907),\n",
       " ('mother', 895),\n",
       " ('lives', 894),\n",
       " ('came', 893),\n",
       " ('stars', 890),\n",
       " ('cinema', 889),\n",
       " ('looks', 885),\n",
       " ('perhaps', 884),\n",
       " ('read', 882),\n",
       " ('enjoyed', 879),\n",
       " ('boy', 875),\n",
       " ('drama', 873),\n",
       " ('highly', 871),\n",
       " ('given', 870),\n",
       " ('playing', 867),\n",
       " ('use', 864),\n",
       " ('next', 859),\n",
       " ('women', 858),\n",
       " ('fine', 857),\n",
       " ('effects', 856),\n",
       " ('kids', 854),\n",
       " ('entertaining', 853),\n",
       " ('need', 852),\n",
       " ('line', 850),\n",
       " ('works', 848),\n",
       " ('someone', 847),\n",
       " ('mr', 836),\n",
       " ('simply', 835),\n",
       " ('picture', 833),\n",
       " ('children', 833),\n",
       " ('face', 831),\n",
       " ('keep', 831),\n",
       " ('friend', 831),\n",
       " ('dark', 830),\n",
       " ('overall', 828),\n",
       " ('certainly', 828),\n",
       " ('minutes', 827),\n",
       " ('wasn', 824),\n",
       " ('history', 822),\n",
       " ('finally', 820),\n",
       " ('couple', 816),\n",
       " ('against', 815),\n",
       " ('son', 809),\n",
       " ('understand', 808),\n",
       " ('lost', 807),\n",
       " ('michael', 805),\n",
       " ('else', 801),\n",
       " ('throughout', 798),\n",
       " ('fans', 797),\n",
       " ('city', 792),\n",
       " ('reason', 789),\n",
       " ('written', 787),\n",
       " ('production', 787),\n",
       " ('several', 784),\n",
       " ('school', 783),\n",
       " ('based', 781),\n",
       " ('rest', 781),\n",
       " ('try', 780),\n",
       " ('dead', 776),\n",
       " ('hope', 775),\n",
       " ('strong', 768),\n",
       " ('white', 765),\n",
       " ('tell', 759),\n",
       " ('itself', 758),\n",
       " ('half', 753),\n",
       " ('person', 749),\n",
       " ('sometimes', 746),\n",
       " ('past', 744),\n",
       " ('start', 744),\n",
       " ('genre', 743),\n",
       " ('beginning', 739),\n",
       " ('final', 739),\n",
       " ('town', 738),\n",
       " ('art', 734),\n",
       " ('humor', 732),\n",
       " ('game', 732),\n",
       " ('yes', 731),\n",
       " ('idea', 731),\n",
       " ('late', 730),\n",
       " ('becomes', 729),\n",
       " ('despite', 729),\n",
       " ('able', 726),\n",
       " ('case', 726),\n",
       " ('money', 723),\n",
       " ('child', 721),\n",
       " ('completely', 721),\n",
       " ('side', 719),\n",
       " ('camera', 716),\n",
       " ('getting', 714),\n",
       " ('instead', 712),\n",
       " ('soon', 702),\n",
       " ('under', 700),\n",
       " ('viewer', 699),\n",
       " ('age', 697),\n",
       " ('days', 696),\n",
       " ('stories', 696),\n",
       " ('felt', 694),\n",
       " ('simple', 694),\n",
       " ('roles', 693),\n",
       " ('video', 688),\n",
       " ('name', 683),\n",
       " ('either', 683),\n",
       " ('doing', 677),\n",
       " ('turns', 674),\n",
       " ('wants', 671),\n",
       " ('close', 671),\n",
       " ('title', 669),\n",
       " ('wrong', 668),\n",
       " ('went', 666),\n",
       " ('james', 665),\n",
       " ('evil', 659),\n",
       " ('budget', 657),\n",
       " ('episodes', 657),\n",
       " ('relationship', 655),\n",
       " ('fantastic', 653),\n",
       " ('piece', 653),\n",
       " ('david', 651),\n",
       " ('turn', 648),\n",
       " ('murder', 646),\n",
       " ('parts', 645),\n",
       " ('brother', 644),\n",
       " ('absolutely', 643),\n",
       " ('head', 643),\n",
       " ('experience', 642),\n",
       " ('eyes', 641),\n",
       " ('sex', 638),\n",
       " ('direction', 637),\n",
       " ('called', 637),\n",
       " ('directed', 636),\n",
       " ('lines', 634),\n",
       " ('behind', 633),\n",
       " ('sort', 632),\n",
       " ('actress', 631),\n",
       " ('lead', 630),\n",
       " ('oscar', 628),\n",
       " ('including', 627),\n",
       " ('example', 627),\n",
       " ('known', 625),\n",
       " ('musical', 625),\n",
       " ('chance', 621),\n",
       " ('score', 620),\n",
       " ('already', 619),\n",
       " ('feeling', 619),\n",
       " ('hit', 619),\n",
       " ('voice', 615),\n",
       " ('moment', 612),\n",
       " ('living', 612),\n",
       " ('low', 610),\n",
       " ('supporting', 610),\n",
       " ('ago', 609),\n",
       " ('themselves', 608),\n",
       " ('reality', 605),\n",
       " ('hilarious', 605),\n",
       " ('jack', 604),\n",
       " ('told', 603),\n",
       " ('hand', 601),\n",
       " ('quality', 600),\n",
       " ('moving', 600),\n",
       " ('dialogue', 600),\n",
       " ('song', 599),\n",
       " ('happy', 599),\n",
       " ('matter', 598),\n",
       " ('paul', 598),\n",
       " ('light', 594),\n",
       " ('future', 593),\n",
       " ('entire', 592),\n",
       " ('finds', 591),\n",
       " ('gave', 589),\n",
       " ('laugh', 587),\n",
       " ('released', 586),\n",
       " ('expect', 584),\n",
       " ('fight', 581),\n",
       " ('particularly', 580),\n",
       " ('cinematography', 579),\n",
       " ('police', 579),\n",
       " ('whose', 578),\n",
       " ('type', 578),\n",
       " ('sound', 578),\n",
       " ('view', 573),\n",
       " ('enjoyable', 573),\n",
       " ('number', 572),\n",
       " ('romantic', 572),\n",
       " ('husband', 572),\n",
       " ('daughter', 572),\n",
       " ('documentary', 571),\n",
       " ('self', 570),\n",
       " ('superb', 569),\n",
       " ('modern', 569),\n",
       " ('took', 569),\n",
       " ('robert', 569),\n",
       " ('mean', 566),\n",
       " ('shown', 563),\n",
       " ('coming', 561),\n",
       " ('important', 560),\n",
       " ('king', 559),\n",
       " ('leave', 559),\n",
       " ('change', 558),\n",
       " ('somewhat', 555),\n",
       " ('wanted', 555),\n",
       " ('tells', 554),\n",
       " ('events', 552),\n",
       " ('run', 552),\n",
       " ('career', 552),\n",
       " ('country', 552),\n",
       " ('heard', 550),\n",
       " ('season', 550),\n",
       " ('greatest', 549),\n",
       " ('girls', 549),\n",
       " ('etc', 547),\n",
       " ('care', 546),\n",
       " ('starts', 545),\n",
       " ('english', 542),\n",
       " ('killer', 541),\n",
       " ('tale', 540),\n",
       " ('guys', 540),\n",
       " ('totally', 540),\n",
       " ('animation', 540),\n",
       " ('usual', 539),\n",
       " ('miss', 535),\n",
       " ('opinion', 535),\n",
       " ('easy', 531),\n",
       " ('violence', 531),\n",
       " ('songs', 530),\n",
       " ('british', 528),\n",
       " ('says', 526),\n",
       " ('realistic', 525),\n",
       " ('writing', 524),\n",
       " ('writer', 522),\n",
       " ('act', 522),\n",
       " ('comic', 521),\n",
       " ('thriller', 519),\n",
       " ('television', 517),\n",
       " ('power', 516),\n",
       " ('ones', 515),\n",
       " ('kid', 514),\n",
       " ('york', 513),\n",
       " ('novel', 513),\n",
       " ('alone', 512),\n",
       " ('problem', 512),\n",
       " ('attention', 509),\n",
       " ('involved', 508),\n",
       " ('kill', 507),\n",
       " ('extremely', 507),\n",
       " ('seemed', 506),\n",
       " ('hero', 505),\n",
       " ('french', 505),\n",
       " ('rock', 504),\n",
       " ('stuff', 501),\n",
       " ('wish', 499),\n",
       " ('begins', 498),\n",
       " ('taken', 497),\n",
       " ('sad', 497),\n",
       " ('ways', 496),\n",
       " ('richard', 495),\n",
       " ('knows', 494),\n",
       " ('atmosphere', 493),\n",
       " ('similar', 491),\n",
       " ('surprised', 491),\n",
       " ('taking', 491),\n",
       " ('car', 491),\n",
       " ('george', 490),\n",
       " ('perfectly', 490),\n",
       " ('across', 489),\n",
       " ('team', 489),\n",
       " ('eye', 489),\n",
       " ('sequence', 489),\n",
       " ('room', 488),\n",
       " ('due', 488),\n",
       " ('among', 488),\n",
       " ('serious', 488),\n",
       " ('powerful', 488),\n",
       " ('strange', 487),\n",
       " ('order', 487),\n",
       " ('cannot', 487),\n",
       " ('b', 487),\n",
       " ('beauty', 486),\n",
       " ('famous', 485),\n",
       " ('happened', 484),\n",
       " ('tries', 484),\n",
       " ('herself', 484),\n",
       " ('myself', 484),\n",
       " ('class', 483),\n",
       " ('four', 482),\n",
       " ('cool', 481),\n",
       " ('release', 479),\n",
       " ('anyway', 479),\n",
       " ('theme', 479),\n",
       " ('opening', 478),\n",
       " ('entertainment', 477),\n",
       " ('slow', 475),\n",
       " ('ends', 475),\n",
       " ('unique', 475),\n",
       " ('exactly', 475),\n",
       " ('easily', 474),\n",
       " ('level', 474),\n",
       " ('o', 474),\n",
       " ('red', 474),\n",
       " ('interest', 472),\n",
       " ('happen', 471),\n",
       " ('crime', 470),\n",
       " ('viewing', 468),\n",
       " ('sets', 467),\n",
       " ('memorable', 467),\n",
       " ('stop', 466),\n",
       " ('group', 466),\n",
       " ('problems', 463),\n",
       " ('dance', 463),\n",
       " ('working', 463),\n",
       " ('sister', 463),\n",
       " ('message', 463),\n",
       " ('knew', 462),\n",
       " ('mystery', 461),\n",
       " ('nature', 461),\n",
       " ('bring', 460),\n",
       " ('believable', 459),\n",
       " ('thinking', 459),\n",
       " ('brought', 459),\n",
       " ('mostly', 458),\n",
       " ('disney', 457),\n",
       " ('couldn', 457),\n",
       " ('society', 456),\n",
       " ('lady', 455),\n",
       " ('within', 455),\n",
       " ('blood', 454),\n",
       " ('parents', 453),\n",
       " ('upon', 453),\n",
       " ('viewers', 453),\n",
       " ('meets', 452),\n",
       " ('form', 452),\n",
       " ('peter', 452),\n",
       " ('tom', 452),\n",
       " ('usually', 452),\n",
       " ('soundtrack', 452),\n",
       " ('local', 450),\n",
       " ('certain', 448),\n",
       " ('follow', 448),\n",
       " ('whether', 447),\n",
       " ('possible', 446),\n",
       " ('emotional', 445),\n",
       " ('killed', 444),\n",
       " ('above', 444),\n",
       " ('de', 444),\n",
       " ('god', 443),\n",
       " ('middle', 443),\n",
       " ('needs', 442),\n",
       " ('happens', 442),\n",
       " ('flick', 442),\n",
       " ('masterpiece', 441),\n",
       " ('period', 440),\n",
       " ('major', 440),\n",
       " ('named', 439),\n",
       " ('haven', 439),\n",
       " ('particular', 438),\n",
       " ('th', 438),\n",
       " ('earth', 437),\n",
       " ('feature', 437),\n",
       " ('stand', 436),\n",
       " ('words', 435),\n",
       " ('typical', 435),\n",
       " ('elements', 433),\n",
       " ('obviously', 433),\n",
       " ('romance', 431),\n",
       " ('jane', 430),\n",
       " ('yourself', 427),\n",
       " ('showing', 427),\n",
       " ('brings', 426),\n",
       " ('fantasy', 426),\n",
       " ('guess', 423),\n",
       " ('america', 423),\n",
       " ('unfortunately', 422),\n",
       " ('huge', 422),\n",
       " ('indeed', 421),\n",
       " ('running', 421),\n",
       " ('talent', 420),\n",
       " ('stage', 419),\n",
       " ('started', 418),\n",
       " ('leads', 417),\n",
       " ('sweet', 417),\n",
       " ('japanese', 417),\n",
       " ('poor', 416),\n",
       " ('deal', 416),\n",
       " ('incredible', 413),\n",
       " ('personal', 413),\n",
       " ('fast', 412),\n",
       " ('became', 410),\n",
       " ('deep', 410),\n",
       " ('hours', 409),\n",
       " ('giving', 408),\n",
       " ('nearly', 408),\n",
       " ('dream', 408),\n",
       " ('clearly', 407),\n",
       " ('turned', 407),\n",
       " ('obvious', 406),\n",
       " ('near', 406),\n",
       " ('cut', 405),\n",
       " ('surprise', 405),\n",
       " ('era', 404),\n",
       " ('body', 404),\n",
       " ('hour', 403),\n",
       " ('female', 403),\n",
       " ('five', 403),\n",
       " ('note', 399),\n",
       " ('learn', 398),\n",
       " ('truth', 398),\n",
       " ('except', 397),\n",
       " ('feels', 397),\n",
       " ('match', 397),\n",
       " ('tony', 397),\n",
       " ('filmed', 394),\n",
       " ('clear', 394),\n",
       " ('complete', 394),\n",
       " ('street', 393),\n",
       " ('eventually', 393),\n",
       " ('keeps', 393),\n",
       " ('older', 393),\n",
       " ('lots', 393),\n",
       " ('buy', 392),\n",
       " ('william', 391),\n",
       " ('stewart', 391),\n",
       " ('fall', 390),\n",
       " ('joe', 390),\n",
       " ('meet', 390),\n",
       " ('unlike', 389),\n",
       " ('talking', 389),\n",
       " ('shots', 389),\n",
       " ('rating', 389),\n",
       " ('difficult', 389),\n",
       " ('dramatic', 388),\n",
       " ('means', 388),\n",
       " ('situation', 386),\n",
       " ('wonder', 386),\n",
       " ('present', 386),\n",
       " ('appears', 386),\n",
       " ('subject', 386),\n",
       " ('comments', 385),\n",
       " ('general', 383),\n",
       " ('sequences', 383),\n",
       " ('lee', 383),\n",
       " ('points', 382),\n",
       " ('earlier', 382),\n",
       " ('gone', 379),\n",
       " ('check', 379),\n",
       " ('suspense', 378),\n",
       " ('recommended', 378),\n",
       " ('ten', 378),\n",
       " ('third', 377),\n",
       " ('business', 377),\n",
       " ('talk', 375),\n",
       " ('leaves', 375),\n",
       " ('beyond', 375),\n",
       " ('portrayal', 374),\n",
       " ('beautifully', 373),\n",
       " ('single', 372),\n",
       " ('bill', 372),\n",
       " ('plenty', 371),\n",
       " ('word', 371),\n",
       " ('whom', 370),\n",
       " ('falls', 370),\n",
       " ('scary', 369),\n",
       " ('non', 369),\n",
       " ('figure', 369),\n",
       " ('battle', 369),\n",
       " ('using', 368),\n",
       " ('return', 368),\n",
       " ('doubt', 367),\n",
       " ('add', 367),\n",
       " ('hear', 366),\n",
       " ('solid', 366),\n",
       " ('success', 366),\n",
       " ('jokes', 365),\n",
       " ('oh', 365),\n",
       " ('touching', 365),\n",
       " ('political', 365),\n",
       " ('hell', 364),\n",
       " ('awesome', 364),\n",
       " ('boys', 364),\n",
       " ('sexual', 362),\n",
       " ('recently', 362),\n",
       " ('dog', 362),\n",
       " ('please', 361),\n",
       " ('wouldn', 361),\n",
       " ('straight', 361),\n",
       " ('features', 361),\n",
       " ('forget', 360),\n",
       " ('setting', 360),\n",
       " ('lack', 360),\n",
       " ('married', 359),\n",
       " ('mark', 359),\n",
       " ('social', 357),\n",
       " ('interested', 356),\n",
       " ('adventure', 356),\n",
       " ('actual', 355),\n",
       " ('terrific', 355),\n",
       " ('sees', 355),\n",
       " ('brothers', 355),\n",
       " ('move', 354),\n",
       " ('call', 354),\n",
       " ('various', 353),\n",
       " ('theater', 353),\n",
       " ('dr', 353),\n",
       " ('animated', 352),\n",
       " ('western', 351),\n",
       " ('baby', 350),\n",
       " ('space', 350),\n",
       " ('leading', 348),\n",
       " ('disappointed', 348),\n",
       " ('portrayed', 346),\n",
       " ('aren', 346),\n",
       " ('screenplay', 345),\n",
       " ('smith', 345),\n",
       " ('towards', 344),\n",
       " ('hate', 344),\n",
       " ('noir', 343),\n",
       " ('outstanding', 342),\n",
       " ('decent', 342),\n",
       " ('kelly', 342),\n",
       " ('directors', 341),\n",
       " ('journey', 341),\n",
       " ('none', 340),\n",
       " ('looked', 340),\n",
       " ('effective', 340),\n",
       " ('storyline', 339),\n",
       " ('caught', 339),\n",
       " ('sci', 339),\n",
       " ('fi', 339),\n",
       " ('cold', 339),\n",
       " ('mary', 339),\n",
       " ('rich', 338),\n",
       " ('charming', 338),\n",
       " ('popular', 337),\n",
       " ('rare', 337),\n",
       " ('manages', 337),\n",
       " ('harry', 337),\n",
       " ('spirit', 336),\n",
       " ('appreciate', 335),\n",
       " ('open', 335),\n",
       " ('moves', 334),\n",
       " ('basically', 334),\n",
       " ('acted', 334),\n",
       " ('inside', 333),\n",
       " ('boring', 333),\n",
       " ('century', 333),\n",
       " ('mention', 333),\n",
       " ('deserves', 333),\n",
       " ('subtle', 333),\n",
       " ('pace', 333),\n",
       " ('familiar', 332),\n",
       " ('background', 332),\n",
       " ('ben', 331),\n",
       " ('creepy', 330),\n",
       " ('supposed', 330),\n",
       " ('secret', 329),\n",
       " ('die', 328),\n",
       " ('jim', 328),\n",
       " ('question', 327),\n",
       " ('effect', 327),\n",
       " ('natural', 327),\n",
       " ('impressive', 326),\n",
       " ('rate', 326),\n",
       " ('language', 326),\n",
       " ('saying', 325),\n",
       " ('intelligent', 325),\n",
       " ('telling', 324),\n",
       " ('realize', 324),\n",
       " ('material', 324),\n",
       " ('scott', 324),\n",
       " ('singing', 323),\n",
       " ('dancing', 322),\n",
       " ('visual', 321),\n",
       " ('adult', 321),\n",
       " ('imagine', 321),\n",
       " ('kept', 320),\n",
       " ('office', 320),\n",
       " ('uses', 319),\n",
       " ('pure', 318),\n",
       " ('wait', 318),\n",
       " ('stunning', 318),\n",
       " ('review', 317),\n",
       " ('previous', 317),\n",
       " ('copy', 317),\n",
       " ('seriously', 317),\n",
       " ('reading', 316),\n",
       " ('create', 316),\n",
       " ('hot', 316),\n",
       " ('created', 316),\n",
       " ('magic', 316),\n",
       " ('somehow', 316),\n",
       " ('stay', 315),\n",
       " ('attempt', 315),\n",
       " ('escape', 315),\n",
       " ('crazy', 315),\n",
       " ('air', 315),\n",
       " ('frank', 315),\n",
       " ('hands', 314),\n",
       " ('filled', 313),\n",
       " ('expected', 312),\n",
       " ('average', 312),\n",
       " ('surprisingly', 312),\n",
       " ('complex', 311),\n",
       " ('quickly', 310),\n",
       " ('successful', 310),\n",
       " ('studio', 310),\n",
       " ('plus', 309),\n",
       " ('male', 309),\n",
       " ('co', 307),\n",
       " ('images', 306),\n",
       " ('casting', 306),\n",
       " ('following', 306),\n",
       " ('minute', 306),\n",
       " ('exciting', 306),\n",
       " ('members', 305),\n",
       " ('follows', 305),\n",
       " ('themes', 305),\n",
       " ('german', 305),\n",
       " ('reasons', 305),\n",
       " ('e', 305),\n",
       " ('touch', 304),\n",
       " ('edge', 304),\n",
       " ('free', 304),\n",
       " ('cute', 304),\n",
       " ('genius', 304),\n",
       " ('outside', 303),\n",
       " ('reviews', 302),\n",
       " ('admit', 302),\n",
       " ('ok', 302),\n",
       " ('younger', 302),\n",
       " ('fighting', 301),\n",
       " ('odd', 301),\n",
       " ('master', 301),\n",
       " ('recent', 300),\n",
       " ('thanks', 300),\n",
       " ('break', 300),\n",
       " ('comment', 300),\n",
       " ('apart', 299),\n",
       " ('emotions', 298),\n",
       " ('lovely', 298),\n",
       " ('begin', 298),\n",
       " ('doctor', 297),\n",
       " ('party', 297),\n",
       " ('italian', 297),\n",
       " ('la', 296),\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 548962),\n",
       " ('.', 167538),\n",
       " ('the', 163389),\n",
       " ('a', 79321),\n",
       " ('and', 74385),\n",
       " ('of', 69009),\n",
       " ('to', 68974),\n",
       " ('br', 52637),\n",
       " ('is', 50083),\n",
       " ('it', 48327),\n",
       " ('i', 46880),\n",
       " ('in', 43753),\n",
       " ('this', 40920),\n",
       " ('that', 37615),\n",
       " ('s', 31546),\n",
       " ('was', 26291),\n",
       " ('movie', 24965),\n",
       " ('for', 21927),\n",
       " ('but', 21781),\n",
       " ('with', 20878),\n",
       " ('as', 20625),\n",
       " ('t', 20361),\n",
       " ('film', 19218),\n",
       " ('you', 17549),\n",
       " ('on', 17192),\n",
       " ('not', 16354),\n",
       " ('have', 15144),\n",
       " ('are', 14623),\n",
       " ('be', 14541),\n",
       " ('he', 13856),\n",
       " ('one', 13134),\n",
       " ('they', 13011),\n",
       " ('\\n', 12500),\n",
       " ('at', 12279),\n",
       " ('his', 12147),\n",
       " ('all', 12036),\n",
       " ('so', 11463),\n",
       " ('like', 11238),\n",
       " ('there', 10775),\n",
       " ('just', 10619),\n",
       " ('by', 10549),\n",
       " ('or', 10272),\n",
       " ('an', 10266),\n",
       " ('who', 9969),\n",
       " ('from', 9731),\n",
       " ('if', 9518),\n",
       " ('about', 9061),\n",
       " ('out', 8979),\n",
       " ('what', 8422),\n",
       " ('some', 8306),\n",
       " ('no', 8143),\n",
       " ('her', 7947),\n",
       " ('even', 7687),\n",
       " ('can', 7653),\n",
       " ('has', 7604),\n",
       " ('good', 7423),\n",
       " ('bad', 7401),\n",
       " ('would', 7036),\n",
       " ('up', 6970),\n",
       " ('only', 6781),\n",
       " ('more', 6730),\n",
       " ('when', 6726),\n",
       " ('she', 6444),\n",
       " ('really', 6262),\n",
       " ('time', 6209),\n",
       " ('had', 6142),\n",
       " ('my', 6015),\n",
       " ('were', 6001),\n",
       " ('which', 5780),\n",
       " ('very', 5764),\n",
       " ('me', 5606),\n",
       " ('see', 5452),\n",
       " ('don', 5336),\n",
       " ('we', 5328),\n",
       " ('their', 5278),\n",
       " ('do', 5236),\n",
       " ('story', 5208),\n",
       " ('than', 5183),\n",
       " ('been', 5100),\n",
       " ('much', 5078),\n",
       " ('get', 5037),\n",
       " ('because', 4966),\n",
       " ('people', 4806),\n",
       " ('then', 4761),\n",
       " ('make', 4722),\n",
       " ('how', 4688),\n",
       " ('could', 4686),\n",
       " ('any', 4658),\n",
       " ('into', 4567),\n",
       " ('made', 4541),\n",
       " ('first', 4306),\n",
       " ('other', 4305),\n",
       " ('well', 4254),\n",
       " ('too', 4174),\n",
       " ('them', 4165),\n",
       " ('plot', 4154),\n",
       " ('movies', 4080),\n",
       " ('acting', 4056),\n",
       " ('will', 3993),\n",
       " ('way', 3989),\n",
       " ('most', 3919),\n",
       " ('him', 3858),\n",
       " ('after', 3838),\n",
       " ('its', 3655),\n",
       " ('think', 3643),\n",
       " ('also', 3608),\n",
       " ('characters', 3600),\n",
       " ('off', 3567),\n",
       " ('watch', 3550),\n",
       " ('character', 3506),\n",
       " ('did', 3506),\n",
       " ('why', 3463),\n",
       " ('being', 3393),\n",
       " ('better', 3358),\n",
       " ('know', 3334),\n",
       " ('over', 3316),\n",
       " ('seen', 3265),\n",
       " ('ever', 3263),\n",
       " ('never', 3259),\n",
       " ('your', 3233),\n",
       " ('where', 3219),\n",
       " ('two', 3173),\n",
       " ('little', 3096),\n",
       " ('films', 3077),\n",
       " ('here', 3027),\n",
       " ('m', 3000),\n",
       " ('nothing', 2990),\n",
       " ('say', 2982),\n",
       " ('end', 2954),\n",
       " ('something', 2942),\n",
       " ('should', 2920),\n",
       " ('many', 2909),\n",
       " ('does', 2871),\n",
       " ('thing', 2866),\n",
       " ('show', 2862),\n",
       " ('ve', 2829),\n",
       " ('scene', 2816),\n",
       " ('scenes', 2785),\n",
       " ('these', 2724),\n",
       " ('go', 2717),\n",
       " ('didn', 2646),\n",
       " ('great', 2640),\n",
       " ('watching', 2640),\n",
       " ('re', 2620),\n",
       " ('doesn', 2601),\n",
       " ('through', 2560),\n",
       " ('such', 2544),\n",
       " ('man', 2516),\n",
       " ('worst', 2480),\n",
       " ('actually', 2449),\n",
       " ('actors', 2437),\n",
       " ('life', 2429),\n",
       " ('back', 2424),\n",
       " ('while', 2418),\n",
       " ('director', 2405),\n",
       " ('funny', 2336),\n",
       " ('going', 2319),\n",
       " ('still', 2283),\n",
       " ('another', 2254),\n",
       " ('look', 2247),\n",
       " ('now', 2237),\n",
       " ('old', 2215),\n",
       " ('those', 2212),\n",
       " ('real', 2170),\n",
       " ('few', 2158),\n",
       " ('love', 2152),\n",
       " ('horror', 2150),\n",
       " ('before', 2147),\n",
       " ('want', 2141),\n",
       " ('minutes', 2126),\n",
       " ('pretty', 2115),\n",
       " ('best', 2094),\n",
       " ('though', 2091),\n",
       " ('same', 2081),\n",
       " ('script', 2074),\n",
       " ('work', 2027),\n",
       " ('every', 2025),\n",
       " ('seems', 2023),\n",
       " ('least', 2011),\n",
       " ('enough', 1997),\n",
       " ('down', 1988),\n",
       " ('original', 1983),\n",
       " ('guy', 1964),\n",
       " ('got', 1952),\n",
       " ('around', 1943),\n",
       " ('part', 1942),\n",
       " ('lot', 1892),\n",
       " ('anything', 1874),\n",
       " ('find', 1860),\n",
       " ('new', 1854),\n",
       " ('again', 1849),\n",
       " ('isn', 1849),\n",
       " ('point', 1845),\n",
       " ('things', 1839),\n",
       " ('fact', 1839),\n",
       " ('give', 1823),\n",
       " ('makes', 1814),\n",
       " ('take', 1800),\n",
       " ('thought', 1798),\n",
       " ('d', 1770),\n",
       " ('whole', 1768),\n",
       " ('long', 1761),\n",
       " ('years', 1759),\n",
       " ('however', 1740),\n",
       " ('gets', 1714),\n",
       " ('making', 1695),\n",
       " ('cast', 1694),\n",
       " ('big', 1662),\n",
       " ('might', 1658),\n",
       " ('interesting', 1648),\n",
       " ('money', 1638),\n",
       " ('us', 1628),\n",
       " ('right', 1625),\n",
       " ('far', 1619),\n",
       " ('quite', 1596),\n",
       " ('without', 1595),\n",
       " ('come', 1595),\n",
       " ('almost', 1574),\n",
       " ('ll', 1567),\n",
       " ('action', 1566),\n",
       " ('awful', 1557),\n",
       " ('kind', 1539),\n",
       " ('reason', 1534),\n",
       " ('am', 1530),\n",
       " ('looks', 1528),\n",
       " ('must', 1522),\n",
       " ('done', 1510),\n",
       " ('comedy', 1504),\n",
       " ('someone', 1490),\n",
       " ('trying', 1486),\n",
       " ('wasn', 1484),\n",
       " ('poor', 1481),\n",
       " ('boring', 1478),\n",
       " ('instead', 1478),\n",
       " ('saw', 1475),\n",
       " ('away', 1469),\n",
       " ('girl', 1463),\n",
       " ('probably', 1444),\n",
       " ('believe', 1434),\n",
       " ('sure', 1433),\n",
       " ('looking', 1430),\n",
       " ('stupid', 1428),\n",
       " ('anyone', 1418),\n",
       " ('times', 1406),\n",
       " ('maybe', 1404),\n",
       " ('world', 1404),\n",
       " ('rather', 1394),\n",
       " ('terrible', 1391),\n",
       " ('may', 1390),\n",
       " ('last', 1390),\n",
       " ('since', 1388),\n",
       " ('let', 1385),\n",
       " ('tv', 1382),\n",
       " ('hard', 1374),\n",
       " ('between', 1374),\n",
       " ('waste', 1358),\n",
       " ('woman', 1356),\n",
       " ('feel', 1354),\n",
       " ('effects', 1348),\n",
       " ('half', 1341),\n",
       " ('own', 1333),\n",
       " ('young', 1317),\n",
       " ('music', 1316),\n",
       " ('idea', 1312),\n",
       " ('sense', 1306),\n",
       " ('bit', 1298),\n",
       " ('having', 1280),\n",
       " ('book', 1278),\n",
       " ('found', 1267),\n",
       " ('put', 1263),\n",
       " ('series', 1263),\n",
       " ('goes', 1256),\n",
       " ('worse', 1249),\n",
       " ('said', 1230),\n",
       " ('comes', 1224),\n",
       " ('role', 1222),\n",
       " ('main', 1220),\n",
       " ('else', 1199),\n",
       " ('everything', 1197),\n",
       " ('yet', 1196),\n",
       " ('low', 1189),\n",
       " ('screen', 1188),\n",
       " ('supposed', 1186),\n",
       " ('actor', 1185),\n",
       " ('either', 1183),\n",
       " ('budget', 1179),\n",
       " ('ending', 1179),\n",
       " ('audience', 1178),\n",
       " ('set', 1177),\n",
       " ('family', 1170),\n",
       " ('left', 1169),\n",
       " ('completely', 1168),\n",
       " ('both', 1158),\n",
       " ('wrong', 1155),\n",
       " ('always', 1151),\n",
       " ('course', 1148),\n",
       " ('place', 1148),\n",
       " ('seem', 1147),\n",
       " ('watched', 1142),\n",
       " ('day', 1132),\n",
       " ('simply', 1130),\n",
       " ('shot', 1126),\n",
       " ('mean', 1117),\n",
       " ('special', 1102),\n",
       " ('dead', 1101),\n",
       " ('three', 1094),\n",
       " ('house', 1085),\n",
       " ('oh', 1084),\n",
       " ('night', 1083),\n",
       " ('read', 1082),\n",
       " ('less', 1067),\n",
       " ('high', 1066),\n",
       " ('year', 1064),\n",
       " ('camera', 1061),\n",
       " ('worth', 1057),\n",
       " ('our', 1056),\n",
       " ('try', 1051),\n",
       " ('horrible', 1046),\n",
       " ('sex', 1046),\n",
       " ('video', 1043),\n",
       " ('black', 1039),\n",
       " ('although', 1036),\n",
       " ('couldn', 1036),\n",
       " ('once', 1033),\n",
       " ('rest', 1022),\n",
       " ('dvd', 1021),\n",
       " ('line', 1018),\n",
       " ('played', 1017),\n",
       " ('fun', 1007),\n",
       " ('during', 1006),\n",
       " ('production', 1003),\n",
       " ('everyone', 1002),\n",
       " ('play', 993),\n",
       " ('mind', 990),\n",
       " ('version', 989),\n",
       " ('kids', 989),\n",
       " ('seeing', 988),\n",
       " ('american', 980),\n",
       " ('given', 978),\n",
       " ('used', 969),\n",
       " ('performance', 968),\n",
       " ('especially', 963),\n",
       " ('together', 963),\n",
       " ('tell', 959),\n",
       " ('women', 958),\n",
       " ('start', 956),\n",
       " ('need', 955),\n",
       " ('second', 953),\n",
       " ('takes', 950),\n",
       " ('each', 950),\n",
       " ('wife', 944),\n",
       " ('dialogue', 942),\n",
       " ('use', 940),\n",
       " ('problem', 938),\n",
       " ('star', 934),\n",
       " ('unfortunately', 931),\n",
       " ('himself', 929),\n",
       " ('doing', 926),\n",
       " ('death', 922),\n",
       " ('name', 921),\n",
       " ('lines', 919),\n",
       " ('killer', 914),\n",
       " ('getting', 913),\n",
       " ('help', 905),\n",
       " ('couple', 902),\n",
       " ('fan', 902),\n",
       " ('head', 898),\n",
       " ('crap', 895),\n",
       " ('guess', 888),\n",
       " ('piece', 884),\n",
       " ('nice', 880),\n",
       " ('different', 878),\n",
       " ('school', 876),\n",
       " ('later', 875),\n",
       " ('entire', 869),\n",
       " ('shows', 860),\n",
       " ('next', 858),\n",
       " ('john', 858),\n",
       " ('short', 857),\n",
       " ('seemed', 857),\n",
       " ('hollywood', 850),\n",
       " ('home', 848),\n",
       " ('true', 846),\n",
       " ('person', 846),\n",
       " ('absolutely', 842),\n",
       " ('sort', 840),\n",
       " ('care', 839),\n",
       " ('understand', 836),\n",
       " ('plays', 835),\n",
       " ('felt', 834),\n",
       " ('written', 829),\n",
       " ('title', 828),\n",
       " ('men', 822),\n",
       " ('until', 821),\n",
       " ('flick', 816),\n",
       " ('decent', 815),\n",
       " ('face', 814),\n",
       " ('friends', 810),\n",
       " ('stars', 807),\n",
       " ('job', 807),\n",
       " ('case', 807),\n",
       " ('itself', 804),\n",
       " ('yes', 801),\n",
       " ('perhaps', 800),\n",
       " ('went', 797),\n",
       " ('wanted', 797),\n",
       " ('called', 796),\n",
       " ('annoying', 795),\n",
       " ('ridiculous', 790),\n",
       " ('tries', 790),\n",
       " ('laugh', 788),\n",
       " ('evil', 787),\n",
       " ('along', 786),\n",
       " ('top', 785),\n",
       " ('hour', 784),\n",
       " ('full', 783),\n",
       " ('came', 780),\n",
       " ('writing', 780),\n",
       " ('keep', 770),\n",
       " ('totally', 767),\n",
       " ('playing', 766),\n",
       " ('god', 765),\n",
       " ('won', 764),\n",
       " ('guys', 763),\n",
       " ('already', 762),\n",
       " ('gore', 757),\n",
       " ('direction', 748),\n",
       " ('save', 746),\n",
       " ('lost', 745),\n",
       " ('example', 744),\n",
       " ('sound', 742),\n",
       " ('war', 741),\n",
       " ('attempt', 735),\n",
       " ('car', 733),\n",
       " ('except', 733),\n",
       " ('moments', 732),\n",
       " ('blood', 732),\n",
       " ('obviously', 730),\n",
       " ('act', 729),\n",
       " ('remember', 728),\n",
       " ('kill', 727),\n",
       " ('truly', 726),\n",
       " ('white', 726),\n",
       " ('father', 726),\n",
       " ('b', 725),\n",
       " ('thinking', 720),\n",
       " ('ok', 716),\n",
       " ('finally', 716),\n",
       " ('turn', 711),\n",
       " ('quality', 701),\n",
       " ('lack', 698),\n",
       " ('style', 694),\n",
       " ('wouldn', 693),\n",
       " ('cheap', 691),\n",
       " ('none', 690),\n",
       " ('kid', 686),\n",
       " ('please', 686),\n",
       " ('boy', 685),\n",
       " ('seriously', 684),\n",
       " ('lead', 680),\n",
       " ('dull', 677),\n",
       " ('children', 676),\n",
       " ('starts', 675),\n",
       " ('stuff', 673),\n",
       " ('hope', 672),\n",
       " ('looked', 670),\n",
       " ('recommend', 669),\n",
       " ('under', 668),\n",
       " ('run', 667),\n",
       " ('killed', 667),\n",
       " ('enjoy', 666),\n",
       " ('others', 666),\n",
       " ('etc', 663),\n",
       " ('myself', 663),\n",
       " ('beginning', 662),\n",
       " ('girls', 662),\n",
       " ('against', 662),\n",
       " ('obvious', 660),\n",
       " ('small', 660),\n",
       " ('hell', 659),\n",
       " ('slow', 657),\n",
       " ('hand', 656),\n",
       " ('wonder', 652),\n",
       " ('lame', 652),\n",
       " ('becomes', 651),\n",
       " ('picture', 651),\n",
       " ('based', 650),\n",
       " ('early', 648),\n",
       " ('behind', 646),\n",
       " ('poorly', 644),\n",
       " ('avoid', 642),\n",
       " ('apparently', 640),\n",
       " ('complete', 640),\n",
       " ('happens', 639),\n",
       " ('anyway', 638),\n",
       " ('classic', 637),\n",
       " ('several', 636),\n",
       " ('despite', 635),\n",
       " ('certainly', 635),\n",
       " ('episode', 635),\n",
       " ('often', 631),\n",
       " ('cut', 630),\n",
       " ('writer', 630),\n",
       " ('mother', 628),\n",
       " ('predictable', 628),\n",
       " ('gave', 628),\n",
       " ('become', 627),\n",
       " ('close', 625),\n",
       " ('fans', 624),\n",
       " ('saying', 621),\n",
       " ('scary', 619),\n",
       " ('stop', 618),\n",
       " ('live', 618),\n",
       " ('wants', 617),\n",
       " ('self', 615),\n",
       " ('mr', 612),\n",
       " ('jokes', 611),\n",
       " ('friend', 611),\n",
       " ('cannot', 610),\n",
       " ('overall', 609),\n",
       " ('cinema', 604),\n",
       " ('child', 603),\n",
       " ('silly', 601),\n",
       " ('beautiful', 596),\n",
       " ('human', 595),\n",
       " ('expect', 594),\n",
       " ('liked', 593),\n",
       " ('happened', 592),\n",
       " ('bunch', 590),\n",
       " ('entertaining', 590),\n",
       " ('actress', 588),\n",
       " ('final', 588),\n",
       " ('says', 584),\n",
       " ('performances', 584),\n",
       " ('turns', 577),\n",
       " ('humor', 577),\n",
       " ('themselves', 576),\n",
       " ('eyes', 576),\n",
       " ('hours', 574),\n",
       " ('happen', 573),\n",
       " ('basically', 572),\n",
       " ('days', 572),\n",
       " ('running', 571),\n",
       " ('involved', 569),\n",
       " ('disappointed', 569),\n",
       " ('call', 569),\n",
       " ('directed', 568),\n",
       " ('group', 568),\n",
       " ('fight', 567),\n",
       " ('daughter', 566),\n",
       " ('talking', 566),\n",
       " ('body', 566),\n",
       " ('badly', 565),\n",
       " ('sorry', 565),\n",
       " ('throughout', 563),\n",
       " ('viewer', 563),\n",
       " ('yourself', 562),\n",
       " ('extremely', 562),\n",
       " ('interest', 561),\n",
       " ('heard', 561),\n",
       " ('violence', 561),\n",
       " ('shots', 559),\n",
       " ('side', 557),\n",
       " ('word', 556),\n",
       " ('art', 555),\n",
       " ('possible', 554),\n",
       " ('dark', 551),\n",
       " ('game', 551),\n",
       " ('hero', 550),\n",
       " ('alone', 549),\n",
       " ('son', 547),\n",
       " ('type', 547),\n",
       " ('leave', 547),\n",
       " ('gives', 546),\n",
       " ('parts', 546),\n",
       " ('single', 546),\n",
       " ('started', 545),\n",
       " ('female', 543),\n",
       " ('rating', 541),\n",
       " ('mess', 541),\n",
       " ('voice', 541),\n",
       " ('aren', 540),\n",
       " ('town', 540),\n",
       " ('drama', 538),\n",
       " ('definitely', 537),\n",
       " ('unless', 536),\n",
       " ('review', 534),\n",
       " ('effort', 533),\n",
       " ('weak', 533),\n",
       " ('able', 533),\n",
       " ('took', 531),\n",
       " ('non', 530),\n",
       " ('five', 530),\n",
       " ('matter', 529),\n",
       " ('usually', 529),\n",
       " ('michael', 528),\n",
       " ('feeling', 526),\n",
       " ('huge', 523),\n",
       " ('sequel', 522),\n",
       " ('soon', 521),\n",
       " ('exactly', 520),\n",
       " ('past', 519),\n",
       " ('turned', 518),\n",
       " ('police', 518),\n",
       " ('tried', 515),\n",
       " ('middle', 513),\n",
       " ('talent', 513),\n",
       " ('genre', 512),\n",
       " ('zombie', 510),\n",
       " ('ends', 509),\n",
       " ('history', 509),\n",
       " ('straight', 503),\n",
       " ('opening', 501),\n",
       " ('serious', 501),\n",
       " ('coming', 501),\n",
       " ('moment', 500),\n",
       " ('lives', 499),\n",
       " ('sad', 499),\n",
       " ('dialog', 498),\n",
       " ('particularly', 498),\n",
       " ('editing', 493),\n",
       " ('clearly', 492),\n",
       " ('beyond', 491),\n",
       " ('earth', 491),\n",
       " ('taken', 490),\n",
       " ('cool', 490),\n",
       " ('level', 489),\n",
       " ('dumb', 489),\n",
       " ('okay', 488),\n",
       " ('major', 487),\n",
       " ('fast', 485),\n",
       " ('premise', 485),\n",
       " ('joke', 484),\n",
       " ('stories', 484),\n",
       " ('wasted', 483),\n",
       " ('minute', 483),\n",
       " ('across', 482),\n",
       " ('mostly', 482),\n",
       " ('rent', 482),\n",
       " ('late', 481),\n",
       " ('falls', 481),\n",
       " ('fails', 481),\n",
       " ('mention', 478),\n",
       " ('theater', 475),\n",
       " ('stay', 472),\n",
       " ('sometimes', 472),\n",
       " ('hit', 468),\n",
       " ('talk', 467),\n",
       " ('fine', 467),\n",
       " ('die', 466),\n",
       " ('storyline', 465),\n",
       " ('pointless', 465),\n",
       " ('taking', 464),\n",
       " ('order', 462),\n",
       " ('brother', 461),\n",
       " ('whatever', 460),\n",
       " ('told', 460),\n",
       " ('wish', 458),\n",
       " ('room', 456),\n",
       " ('career', 455),\n",
       " ('appears', 455),\n",
       " ('write', 455),\n",
       " ('known', 454),\n",
       " ('husband', 454),\n",
       " ('living', 451),\n",
       " ('sit', 450),\n",
       " ('ten', 450),\n",
       " ('words', 449),\n",
       " ('monster', 448),\n",
       " ('chance', 448),\n",
       " ('hate', 444),\n",
       " ('novel', 444),\n",
       " ('add', 443),\n",
       " ('english', 443),\n",
       " ('somehow', 441),\n",
       " ('strange', 440),\n",
       " ('imdb', 438),\n",
       " ('actual', 438),\n",
       " ('total', 437),\n",
       " ('material', 437),\n",
       " ('killing', 437),\n",
       " ('ones', 437),\n",
       " ('knew', 436),\n",
       " ('king', 434),\n",
       " ('number', 434),\n",
       " ('using', 433),\n",
       " ('lee', 431),\n",
       " ('power', 431),\n",
       " ('shown', 431),\n",
       " ('works', 431),\n",
       " ('giving', 431),\n",
       " ('points', 430),\n",
       " ('possibly', 430),\n",
       " ('kept', 430),\n",
       " ('four', 429),\n",
       " ('local', 427),\n",
       " ('usual', 426),\n",
       " ('including', 425),\n",
       " ('problems', 424),\n",
       " ('ago', 424),\n",
       " ('opinion', 424),\n",
       " ('nudity', 423),\n",
       " ('age', 422),\n",
       " ('due', 421),\n",
       " ('roles', 420),\n",
       " ('writers', 419),\n",
       " ('decided', 419),\n",
       " ('near', 418),\n",
       " ('flat', 418),\n",
       " ('easily', 418),\n",
       " ('murder', 417),\n",
       " ('experience', 417),\n",
       " ('reviews', 416),\n",
       " ('imagine', 415),\n",
       " ('feels', 413),\n",
       " ('plain', 411),\n",
       " ('somewhat', 411),\n",
       " ('class', 410),\n",
       " ('score', 410),\n",
       " ('song', 409),\n",
       " ('bring', 409),\n",
       " ('whether', 409),\n",
       " ('otherwise', 408),\n",
       " ('whose', 408),\n",
       " ('average', 408),\n",
       " ('pathetic', 407),\n",
       " ('nearly', 407),\n",
       " ('knows', 407),\n",
       " ('zombies', 407),\n",
       " ('cinematography', 406),\n",
       " ('cheesy', 406),\n",
       " ('upon', 406),\n",
       " ('city', 405),\n",
       " ('space', 405),\n",
       " ('credits', 404),\n",
       " ('james', 403),\n",
       " ('lots', 403),\n",
       " ('change', 403),\n",
       " ('entertainment', 402),\n",
       " ('nor', 402),\n",
       " ('wait', 401),\n",
       " ('released', 400),\n",
       " ('needs', 399),\n",
       " ('shame', 398),\n",
       " ('attention', 396),\n",
       " ('comments', 394),\n",
       " ('bored', 393),\n",
       " ('free', 393),\n",
       " ('lady', 393),\n",
       " ('expected', 392),\n",
       " ('needed', 392),\n",
       " ('clear', 392),\n",
       " ('view', 391),\n",
       " ('development', 390),\n",
       " ('check', 390),\n",
       " ('doubt', 390),\n",
       " ('figure', 389),\n",
       " ('mystery', 389),\n",
       " ('excellent', 388),\n",
       " ('garbage', 388),\n",
       " ('sequence', 386),\n",
       " ('television', 386),\n",
       " ('o', 385),\n",
       " ('sets', 385),\n",
       " ('laughable', 384),\n",
       " ('potential', 384),\n",
       " ('robert', 382),\n",
       " ('light', 382),\n",
       " ('country', 382),\n",
       " ('documentary', 382),\n",
       " ('reality', 382),\n",
       " ('general', 381),\n",
       " ('ask', 381),\n",
       " ('comic', 380),\n",
       " ('fall', 380),\n",
       " ('begin', 380),\n",
       " ('footage', 379),\n",
       " ('stand', 379),\n",
       " ('forced', 379),\n",
       " ('trash', 379),\n",
       " ('remake', 379),\n",
       " ('thriller', 378),\n",
       " ('songs', 378),\n",
       " ('gay', 377),\n",
       " ('within', 377),\n",
       " ('hardly', 376),\n",
       " ('above', 375),\n",
       " ('gone', 375),\n",
       " ('george', 374),\n",
       " ('means', 373),\n",
       " ('sounds', 373),\n",
       " ('directing', 372),\n",
       " ('move', 372),\n",
       " ('david', 372),\n",
       " ('buy', 372),\n",
       " ('rock', 371),\n",
       " ('forward', 371),\n",
       " ('important', 371),\n",
       " ('hot', 370),\n",
       " ('haven', 370),\n",
       " ('filmed', 370),\n",
       " ('british', 370),\n",
       " ('heart', 369),\n",
       " ('reading', 369),\n",
       " ('fake', 369),\n",
       " ('incredibly', 368),\n",
       " ('weird', 368),\n",
       " ('hear', 368),\n",
       " ('enjoyed', 367),\n",
       " ('hilarious', 367),\n",
       " ('cop', 367),\n",
       " ('musical', 367),\n",
       " ('message', 366),\n",
       " ('happy', 366),\n",
       " ('pay', 366),\n",
       " ('laughs', 365),\n",
       " ('box', 365),\n",
       " ('suspense', 363),\n",
       " ('sadly', 363),\n",
       " ('eye', 362),\n",
       " ('third', 361),\n",
       " ('similar', 361),\n",
       " ('named', 361),\n",
       " ('modern', 360),\n",
       " ('failed', 359),\n",
       " ('events', 359),\n",
       " ('forget', 358),\n",
       " ('question', 358),\n",
       " ('male', 357),\n",
       " ('finds', 357),\n",
       " ('perfect', 356),\n",
       " ('spent', 355),\n",
       " ('sister', 355),\n",
       " ('feature', 354),\n",
       " ('result', 354),\n",
       " ('comment', 353),\n",
       " ('girlfriend', 353),\n",
       " ('sexual', 352),\n",
       " ('attempts', 351),\n",
       " ('neither', 351),\n",
       " ('richard', 351),\n",
       " ('screenplay', 350),\n",
       " ('elements', 350),\n",
       " ('spoilers', 349),\n",
       " ('brain', 348),\n",
       " ('filmmakers', 348),\n",
       " ('showing', 348),\n",
       " ('miss', 347),\n",
       " ('dr', 347),\n",
       " ('christmas', 347),\n",
       " ('cover', 345),\n",
       " ('red', 344),\n",
       " ('sequences', 344),\n",
       " ('typical', 343),\n",
       " ('excuse', 343),\n",
       " ('crazy', 342),\n",
       " ('ideas', 342),\n",
       " ('baby', 342),\n",
       " ('loved', 341),\n",
       " ('meant', 341),\n",
       " ('worked', 340),\n",
       " ('fire', 340),\n",
       " ('unbelievable', 339),\n",
       " ('follow', 339),\n",
       " ('theme', 337),\n",
       " ('barely', 336),\n",
       " ('producers', 336),\n",
       " ('twist', 336),\n",
       " ('plus', 336),\n",
       " ('appear', 336),\n",
       " ('directors', 335),\n",
       " ('team', 335),\n",
       " ('viewers', 333),\n",
       " ('leads', 332),\n",
       " ('tom', 332),\n",
       " ('slasher', 332),\n",
       " ('wrote', 331),\n",
       " ('villain', 331),\n",
       " ('gun', 331),\n",
       " ('working', 331),\n",
       " ('island', 330),\n",
       " ('strong', 330),\n",
       " ('open', 330),\n",
       " ('realize', 330),\n",
       " ('positive', 329),\n",
       " ('disappointing', 329),\n",
       " ('yeah', 329),\n",
       " ('quickly', 329),\n",
       " ('weren', 328),\n",
       " ('release', 328),\n",
       " ('simple', 328),\n",
       " ('honestly', 328),\n",
       " ('eventually', 327),\n",
       " ('period', 327),\n",
       " ('tells', 327),\n",
       " ('kills', 327),\n",
       " ('doctor', 327),\n",
       " ('nowhere', 326),\n",
       " ('list', 326),\n",
       " ('acted', 326),\n",
       " ('herself', 326),\n",
       " ('dog', 326),\n",
       " ('walk', 325),\n",
       " ('air', 324),\n",
       " ('apart', 324),\n",
       " ('makers', 323),\n",
       " ('subject', 323),\n",
       " ('learn', 322),\n",
       " ('fi', 322),\n",
       " ('sci', 319),\n",
       " ('bother', 319),\n",
       " ('admit', 319),\n",
       " ('jack', 318),\n",
       " ('disappointment', 318),\n",
       " ('hands', 318),\n",
       " ('note', 318),\n",
       " ('certain', 317),\n",
       " ('e', 317),\n",
       " ('value', 317),\n",
       " ('casting', 317),\n",
       " ('grade', 316),\n",
       " ('peter', 316),\n",
       " ('suddenly', 315),\n",
       " ('missing', 315),\n",
       " ('form', 313),\n",
       " ('stick', 313),\n",
       " ('previous', 313),\n",
       " ('break', 313),\n",
       " ('soundtrack', 312),\n",
       " ('surprised', 311),\n",
       " ('front', 311),\n",
       " ('expecting', 311),\n",
       " ('parents', 310),\n",
       " ('surprise', 310),\n",
       " ('relationship', 310),\n",
       " ('shoot', 309),\n",
       " ('today', 309),\n",
       " ('painful', 308),\n",
       " ('ways', 308),\n",
       " ('leaves', 308),\n",
       " ('ended', 308),\n",
       " ('creepy', 308),\n",
       " ('concept', 308),\n",
       " ('somewhere', 308),\n",
       " ('vampire', 308),\n",
       " ('spend', 307),\n",
       " ('th', 307),\n",
       " ('future', 306),\n",
       " ('difficult', 306),\n",
       " ('effect', 306),\n",
       " ('fighting', 306),\n",
       " ('street', 306),\n",
       " ('c', 305),\n",
       " ('america', 305),\n",
       " ('accent', 304),\n",
       " ('truth', 302),\n",
       " ('project', 302),\n",
       " ('joe', 301),\n",
       " ('f', 301),\n",
       " ('deal', 301),\n",
       " ('indeed', 301),\n",
       " ('biggest', 300),\n",
       " ('rate', 300),\n",
       " ('paul', 299),\n",
       " ('japanese', 299),\n",
       " ('utterly', 298),\n",
       " ('begins', 298),\n",
       " ('redeeming', 298),\n",
       " ('college', 298),\n",
       " ('york', 297),\n",
       " ('fairly', 297),\n",
       " ('disney', 297),\n",
       " ('crew', 296),\n",
       " ('create', 296),\n",
       " ('cartoon', 296),\n",
       " ('revenge', 296),\n",
       " ('co', 295),\n",
       " ('outside', 295),\n",
       " ('computer', 295),\n",
       " ('interested', 295),\n",
       " ('stage', 295),\n",
       " ('considering', 294),\n",
       " ('speak', 294),\n",
       " ('among', 294),\n",
       " ('towards', 293),\n",
       " ('channel', 293),\n",
       " ('sick', 293),\n",
       " ('talented', 292),\n",
       " ('cause', 292),\n",
       " ('particular', 292),\n",
       " ('van', 292),\n",
       " ('hair', 292),\n",
       " ('bottom', 291),\n",
       " ('reasons', 291),\n",
       " ('mediocre', 290),\n",
       " ('cat', 290),\n",
       " ('telling', 290),\n",
       " ('supporting', 289),\n",
       " ('store', 289),\n",
       " ('hoping', 288),\n",
       " ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_counts.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is just to show the most common words in the positive and negative sentences. However, there are a lot of unnecessary words like `the`, `a`, `was`, and so on. Can you find a way to show the relevant words and not these words? \n",
    "\n",
    "```\n",
    "Hint: Stop Words removal or normalizing each term.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell',\n",
       " 'high',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " 'it',\n",
       " 'ran',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'as',\n",
       " 'some',\n",
       " 'other',\n",
       " 'programs',\n",
       " 'about',\n",
       " 'school',\n",
       " 'life',\n",
       " 'such',\n",
       " 'as',\n",
       " 'teachers',\n",
       " 'my',\n",
       " 'years',\n",
       " 'in',\n",
       " 'the',\n",
       " 'teaching',\n",
       " 'profession',\n",
       " 'lead',\n",
       " 'me']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21025,\n",
       " 308,\n",
       " 6,\n",
       " 3,\n",
       " 1050,\n",
       " 207,\n",
       " 8,\n",
       " 2138,\n",
       " 32,\n",
       " 1,\n",
       " 171,\n",
       " 57,\n",
       " 15,\n",
       " 49,\n",
       " 81,\n",
       " 5785,\n",
       " 44,\n",
       " 382,\n",
       " 110,\n",
       " 140,\n",
       " 15,\n",
       " 5194,\n",
       " 60,\n",
       " 154,\n",
       " 9,\n",
       " 1,\n",
       " 4975,\n",
       " 5852,\n",
       " 475,\n",
       " 71]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab_to_int[word] for word in words[:30]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21025"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int['bromwell']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot encoding\n",
    "\n",
    "We need one hot encoding for the labels. Think of a reason why we need one hot encoded labels for classes?\n",
    "\n",
    "## Task 3: Create one hot encoding for the labels. \n",
    "\n",
    "* Write the one hot encoding logic in the `one_hot` function.\n",
    "* Use 1 for positive label and 0 for negative label.\n",
    "* Save all the values in the `encoded_labels` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 for positive label and 0 for negative label\n",
    "def one_hot(labels):\n",
    "    labels_split = ''.join(labels).split('\\n')\n",
    "    return np.array([1 if label == 'positive' else 0 for label in labels_split])\n",
    "encoded_labels = one_hot(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25001"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the length of your label and uncomment next line only if the encoded_labels size is 25001.\n",
    "# If you dont get the intuition behind this step, print encoded_labels to see it.\n",
    "#encoded_labels = encoded_labels[:25000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_ints = []\n",
    "for review in reviews_split:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in review.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 1\n",
      "Maximum review length: 2514\n"
     ]
    }
   ],
   "source": [
    "# This step is to see if any review is empty and we remove it. Otherwise the input will be all zeroes.\n",
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews before removing outliers:  25001\n",
      "Number of reviews after removing outliers:  25000\n"
     ]
    }
   ],
   "source": [
    "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
    "\n",
    "## remove any reviews/labels with zero length from the reviews_ints list.\n",
    "\n",
    "# get indices of any reviews with length 0\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "\n",
    "# remove 0-length reviews and their labels\n",
    "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
    "\n",
    "print('Number of reviews after removing outliers: ', len(reviews_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Padding the data\n",
    "\n",
    "> Define a function that returns an array `features` that contains the padded data, of a standard size, that we'll pass to the network. \n",
    "* The data should come from `review_ints`, since we want to feed integers to the network. \n",
    "* Each row should be `seq_length` elements long. \n",
    "* For reviews shorter than `seq_length` words, **left pad** with 0s. That is, if the review is `['best', 'movie', 'ever']`, `[117, 18, 128]` as integers, the row will look like `[0, 0, 0, ..., 0, 117, 18, 128]`. \n",
    "* For reviews longer than `seq_length`, use only the first `seq_length` words as the feature vector.\n",
    "\n",
    "As a small example, if the `seq_length=10` and an input review is: \n",
    "```\n",
    "[117, 18, 128]\n",
    "```\n",
    "The resultant, padded sequence should be: \n",
    "\n",
    "```\n",
    "[0, 0, 0, 0, 0, 0, 0, 117, 18, 128]\n",
    "```\n",
    "\n",
    "**Your final `features` array should be a 2D array, with as many rows as there are reviews, and as many columns as the specified `seq_length`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the logic for padding the data\n",
    "def pad_features(reviews_ints, seq_length):\n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "\n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [22382    42 46418    15   706 17139  3389    47    77    35]\n",
      " [ 4505   505    15     3  3342   162  8312  1652     6  4819]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   54    10    14   116    60   798   552    71   364     5]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    1   330   578    34     3   162   748  2731     9   325]\n",
      " [    9    11 10171  5305  1946   689   444    22   280   673]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    1   307 10399  2069  1565  6202  6528  3288 17946 10628]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   21   122  2069  1565   515  8181    88     6  1325  1182]\n",
      " [    1    20     6    76    40     6    58    81    95     5]\n",
      " [   54    10    84   329 26230 46427    63    10    14   614]\n",
      " [   11    20     6    30  1436 32317  3769   690 15100     6]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   40    26   109 17952  1422     9     1   327     4   125]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   10   499     1   307 10399    55    74     8    13    30]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Verify if everything till now is correct. \n",
    "\n",
    "seq_length = 200\n",
    "\n",
    "features = pad_features(reviews_ints, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 10 values of the first 30 batches \n",
    "print(features[:30,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything ready. It's time to split our dataset into `Train`, `Test` and `Validate`. \n",
    "\n",
    "Read more about the train-test-split here : https://cs230-stanford.github.io/train-dev-test-split.html\n",
    "\n",
    "## Task 5: Lets create train, test and val split in the ratio of 8:1:1.  \n",
    "\n",
    "Hint: Either use shuffle and slicing in Python or use train-test-val split in Sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.8\n",
    "val_frac = 0.1\n",
    "test_frac = 0.1\n",
    "\n",
    "\n",
    "def train_test_val_split(features):\n",
    "    return (features[:int(len(features)*train_frac)], \n",
    "features[int(len(features)*train_frac) : (int(len(features)*train_frac) + int(len(features)*val_frac))], \n",
    "features[(int(len(features)*train_frac) + int(len(features)*val_frac)):])\n",
    "    \n",
    "def train_test_val_labels(encoded_labels):\n",
    "    return (encoded_labels[:int(len(encoded_labels)*train_frac)], \n",
    "encoded_labels[int(len(encoded_labels)*train_frac) : (int(len(encoded_labels)*train_frac) + int(len(encoded_labels)*val_frac))], \n",
    "encoded_labels[(int(len(encoded_labels)*train_frac) + int(len(encoded_labels)*val_frac)):])\n",
    "\n",
    "train_x, val_x, test_x = train_test_val_split(features)\n",
    "train_y, val_y, test_y = train_test_val_labels(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(20000, 200) \n",
      "Validation set: \t(2500, 200) \n",
      "Test set: \t\t(2500, 200)\n"
     ]
    }
   ],
   "source": [
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders and Batching\n",
    "\n",
    "After creating training, test, and validation data, we can create DataLoaders for this data by following two steps:\n",
    "1. Create a known format for accessing our data, using [TensorDataset](https://pytorch.org/docs/stable/data.html#) which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n",
    "2. Create DataLoaders and batch our training, validation, and test Tensor datasets.\n",
    "\n",
    "```\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "```\n",
    "\n",
    "This is an alternative to creating a generator function for batching our data into full batches.\n",
    "\n",
    "### Task 6: Create a generator function for the dataset. \n",
    "See the above link for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets for train, test and val\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50 \n",
    "\n",
    "# make sure to SHUFFLE your training data. Keep Shuffle=True.\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 200])\n",
      "Sample input: \n",
      " tensor([[     0,      0,      0,  ...,      1,    814,    341],\n",
      "        [     0,      0,      0,  ...,   3921,     13,   1937],\n",
      "        [     0,      0,      0,  ...,      3,   2663,    226],\n",
      "        ...,\n",
      "        [    10,    139,    114,  ...,      6,     11,     35],\n",
      "        [     0,      0,      0,  ...,     10,   2326,      8],\n",
      "        [    34,   2640,     40,  ...,    328,      3,    190]])\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([ 1,  1,  0,  1,  0,  0,  1,  1,  0,  1,  0,  0,  1,  0,\n",
      "         1,  0,  1,  1,  1,  0,  0,  0,  0,  1,  1,  0,  1,  0,\n",
      "         0,  1,  1,  0,  1,  1,  1,  0,  0,  1,  1,  0,  0,  1,\n",
      "         0,  0,  1,  1,  0,  0,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data and label. \n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available.\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model \n",
    "\n",
    "Here we are creating a simple RNN in PyTorch and pass the output to the a Linear layer and Sigmoid at the end to get the probability score and prediction as POSITIVE or NEGATIVE. \n",
    "\n",
    "The network is very similar to the CNN network created in Exercise 2. \n",
    "\n",
    "More info available at: https://pytorch.org/docs/0.3.1/nn.html?highlight=rnn#torch.nn.RNN\n",
    "\n",
    "Read about the parameters that the RNN takes and see what will happen when `batch_first` is set as `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # RNN layer\n",
    "        self.rnn = nn.RNN(vocab_size, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # RNN out layer\n",
    "        rnn_out, hidden = self.rnn(x, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        rnn_out = rnn_out.view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(rnn_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 7 : Know the shape\n",
    "\n",
    "Given a batch of 64 and input size as 1 and a sequence length of 200 to a RNN with 2 stacked layers and 512 hidden layers, find the shape of input data (x) and the hidden dimension (hidden) specified in the forward pass of the network. Note, the batch_first is kept to be True. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (rnn): RNN(74073, 256, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shri/condaenv/anaconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 1\n",
    "hidden_dim = 256\n",
    "n_layers = 1\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Task 8: LSTM \n",
    "\n",
    "Before we start creating the LSTM, it is important to understand LSTM and to know why we prefer LSTM over a Vanilla RNN for this task. \n",
    "> Here are some good links to know about LSTM:\n",
    "* [Colah Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "* [Understanding LSTM](http://blog.echen.me/2017/05/30/exploring-lstms/)\n",
    "* [RNN effectiveness](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "\n",
    "Now create a class named SentimentLSTM with `n_layers=2`, and rest all hyperparameters same as before. Also, create an embedding layer and feed the output of the embedding layer as input to the LSTM model. Dont forget to add a regularizer (dropout) layer after the LSTM layer with p=0.4 to prevent overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the network\n",
    "\n",
    "Here, we'll instantiate the network. First up, defining the hyperparameters.\n",
    "\n",
    "* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n",
    "* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
    "* `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.\n",
    "* `hidden_dim`: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
    "* `n_layers`: Number of LSTM layers in the network. Typically between 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentLSTM(\n",
      "  (embedding): Embedding(74073, 300)\n",
      "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with these hyperparameters\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 1\n",
    "embedding_dim = 300\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 9: Loss Functions\n",
    "We are using `BCELoss (Binary Cross Entropy Loss)` since we have two output classes. \n",
    "\n",
    "Can Cross Entropy Loss be used instead of BCELoss? \n",
    "\n",
    "If no, why not? If yes, how?\n",
    "\n",
    "Is `NLLLoss()` and last layer as `LogSoftmax()` is same as using `CrossEntropyLoss()` with a Softmax final layer? Can you get the mathematical intuition behind it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 100... Loss: 0.643695... Val Loss: 0.655541\n",
      "Epoch: 1/4... Step: 200... Loss: 0.660986... Val Loss: 0.642946\n",
      "Epoch: 1/4... Step: 300... Loss: 0.623185... Val Loss: 0.572274\n",
      "Epoch: 1/4... Step: 400... Loss: 0.557997... Val Loss: 0.571232\n",
      "Epoch: 2/4... Step: 500... Loss: 0.423477... Val Loss: 0.522310\n",
      "Epoch: 2/4... Step: 600... Loss: 0.460323... Val Loss: 0.507332\n",
      "Epoch: 2/4... Step: 700... Loss: 0.491676... Val Loss: 0.473734\n",
      "Epoch: 2/4... Step: 800... Loss: 0.355832... Val Loss: 0.498556\n",
      "Epoch: 3/4... Step: 900... Loss: 0.391010... Val Loss: 0.558128\n",
      "Epoch: 3/4... Step: 1000... Loss: 0.343864... Val Loss: 0.446501\n",
      "Epoch: 3/4... Step: 1100... Loss: 0.474332... Val Loss: 0.503039\n",
      "Epoch: 3/4... Step: 1200... Loss: 0.307275... Val Loss: 0.468682\n",
      "Epoch: 4/4... Step: 1300... Loss: 0.328526... Val Loss: 0.495935\n",
      "Epoch: 4/4... Step: 1400... Loss: 0.238040... Val Loss: 0.488728\n",
      "Epoch: 4/4... Step: 1500... Loss: 0.343387... Val Loss: 0.513419\n",
      "Epoch: 4/4... Step: 1600... Loss: 0.161433... Val Loss: 0.445079\n"
     ]
    }
   ],
   "source": [
    "#Training and Validation\n",
    "\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.438\n",
      "Test accuracy: 0.809\n"
     ]
    }
   ],
   "source": [
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "Once we are done with training and validating, we can improve training loss and validation loss by playing around with the hyperparameters. Can you find a better set of hyperparams? Play around with it. \n",
    "\n",
    "### Task 10: Prediction Function\n",
    "Now write a prediction function to predict the output for the test set created. Save the results in a CSV file with one column as the reviews and the prediction in the next column. Calculate the accuracy of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative test review\n",
    "test_review_neg = 'Terrible movie.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[388, 18]]\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def tokenize_review(test_review):\n",
    "    test_review = test_review.lower() # lowercase\n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
    "\n",
    "    return test_ints\n",
    "\n",
    "# test code and generate tokenized review\n",
    "test_ints = tokenize_review(test_review_neg)\n",
    "print(test_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  388  18]]\n"
     ]
    }
   ],
   "source": [
    "# test sequence padding\n",
    "seq_length=200\n",
    "features = pad_features(test_ints, seq_length)\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200])\n"
     ]
    }
   ],
   "source": [
    "# test conversion to tensor and pass into your model\n",
    "feature_tensor = torch.from_numpy(features)\n",
    "print(feature_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_review, sequence_length=200):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # tokenize review\n",
    "    test_ints = tokenize_review(test_review)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    # printing output value, before rounding\n",
    "    print('Prediction value: {:.6f}'.format(output.item()))\n",
    "    \n",
    "    # print custom response\n",
    "    if(pred.item()==1):\n",
    "        print(\"Positive review!\")\n",
    "    else:\n",
    "        print(\"Negative review!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value: 0.040737\n",
      "Negative review!\n"
     ]
    }
   ],
   "source": [
    "# call function\n",
    "seq_length=200 # good to use the length that was trained on\n",
    "\n",
    "predict(net, test_review_neg, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question: Create an app using Flask\n",
    "\n",
    "> Extra bonus points if someone attempts this question:\n",
    "* Save the trained model checkpoints.\n",
    "* Create a Flask app and load the model. A similar work in the field of CNN has been done here : https://github.com/kumar-shridhar/Business-Card-Detector (Check `app.py`)\n",
    "* You can use hosting services like Heroku and/or with Docker to host your app and show it to everyone. \n",
    "Example here: https://github.com/selimrbd/sentiment_analysis/blob/master/Dockerfile\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
